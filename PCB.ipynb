{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbvMlHd_QwMG",
        "outputId": "2b5956db-5ced-439e-f1d6-3168f2bff8c9",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.2.60 ðŸš€ Python-3.10.12 torch-2.3.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Setup complete âœ… (2 CPUs, 12.7 GB RAM, 30.1/201.2 GB disk)\n"
          ]
        }
      ],
      "source": [
        "%pip install ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0U_raOOHLalU",
        "outputId": "84cce6b0-f236-4c7b-d0ef-fe18cf8005a4",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing datatraffic.yaml\n"
          ]
        }
      ],
      "source": [
        "%%writefile datatraffic.yaml\n",
        "\n",
        "train: train/images\n",
        "val: valid/images\n",
        "\n",
        "nc: 7\n",
        "names: [\"IC\", \"SMD\", \"capacitor\", \"inductor\", \"other\", \"resistor\", \"transistor\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IRdaa93g37G",
        "outputId": "1f5624f0-cf16-4fa4-f28b-b7e57079c368",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /usr/local/lib/python3.10/dist-packages/ultralytics/engine/trainer.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /usr/local/lib/python3.10/dist-packages/ultralytics/engine/trainer.py\n",
        "# Ultralytics YOLO ðŸš€, AGPL-3.0 license\n",
        "\"\"\"\n",
        "Train a model on a dataset.\n",
        "\n",
        "Usage:\n",
        "    $ yolo mode=train model=yolov8n.pt data=coco128.yaml imgsz=640 epochs=100 batch=16\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "import warnings\n",
        "from copy import deepcopy\n",
        "from datetime import datetime, timedelta\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import distributed as dist\n",
        "from torch import nn, optim\n",
        "\n",
        "from ultralytics.cfg import get_cfg, get_save_dir\n",
        "from ultralytics.data.utils import check_cls_dataset, check_det_dataset\n",
        "from ultralytics.nn.tasks import attempt_load_one_weight, attempt_load_weights\n",
        "from ultralytics.utils import (\n",
        "    DEFAULT_CFG,\n",
        "    LOGGER,\n",
        "    RANK,\n",
        "    TQDM,\n",
        "    __version__,\n",
        "    callbacks,\n",
        "    clean_url,\n",
        "    colorstr,\n",
        "    emojis,\n",
        "    yaml_save,\n",
        ")\n",
        "from ultralytics.utils.autobatch import check_train_batch_size\n",
        "from ultralytics.utils.checks import check_amp, check_file, check_imgsz, check_model_file_from_stem, print_args\n",
        "from ultralytics.utils.dist import ddp_cleanup, generate_ddp_command\n",
        "from ultralytics.utils.files import get_latest_run\n",
        "from ultralytics.utils.torch_utils import (\n",
        "    EarlyStopping,\n",
        "    ModelEMA,\n",
        "    de_parallel,\n",
        "    init_seeds,\n",
        "    one_cycle,\n",
        "    select_device,\n",
        "    strip_optimizer,\n",
        ")\n",
        "\n",
        "\n",
        "class BaseTrainer:\n",
        "    \"\"\"\n",
        "    BaseTrainer.\n",
        "\n",
        "    A base class for creating trainers.\n",
        "\n",
        "    Attributes:\n",
        "        args (SimpleNamespace): Configuration for the trainer.\n",
        "        validator (BaseValidator): Validator instance.\n",
        "        model (nn.Module): Model instance.\n",
        "        callbacks (defaultdict): Dictionary of callbacks.\n",
        "        save_dir (Path): Directory to save results.\n",
        "        wdir (Path): Directory to save weights.\n",
        "        last (Path): Path to the last checkpoint.\n",
        "        best (Path): Path to the best checkpoint.\n",
        "        save_period (int): Save checkpoint every x epochs (disabled if < 1).\n",
        "        batch_size (int): Batch size for training.\n",
        "        epochs (int): Number of epochs to train for.\n",
        "        start_epoch (int): Starting epoch for training.\n",
        "        device (torch.device): Device to use for training.\n",
        "        amp (bool): Flag to enable AMP (Automatic Mixed Precision).\n",
        "        scaler (amp.GradScaler): Gradient scaler for AMP.\n",
        "        data (str): Path to data.\n",
        "        trainset (torch.utils.data.Dataset): Training dataset.\n",
        "        testset (torch.utils.data.Dataset): Testing dataset.\n",
        "        ema (nn.Module): EMA (Exponential Moving Average) of the model.\n",
        "        resume (bool): Resume training from a checkpoint.\n",
        "        lf (nn.Module): Loss function.\n",
        "        scheduler (torch.optim.lr_scheduler._LRScheduler): Learning rate scheduler.\n",
        "        best_fitness (float): The best fitness value achieved.\n",
        "        fitness (float): Current fitness value.\n",
        "        loss (float): Current loss value.\n",
        "        tloss (float): Total loss value.\n",
        "        loss_names (list): List of loss names.\n",
        "        csv (Path): Path to results CSV file.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):\n",
        "        \"\"\"\n",
        "        Initializes the BaseTrainer class.\n",
        "\n",
        "        Args:\n",
        "            cfg (str, optional): Path to a configuration file. Defaults to DEFAULT_CFG.\n",
        "            overrides (dict, optional): Configuration overrides. Defaults to None.\n",
        "        \"\"\"\n",
        "        self.args = get_cfg(cfg, overrides)\n",
        "        self.check_resume(overrides)\n",
        "        self.device = select_device(self.args.device, self.args.batch)\n",
        "        self.validator = None\n",
        "        self.metrics = None\n",
        "        self.plots = {}\n",
        "        init_seeds(self.args.seed + 1 + RANK, deterministic=self.args.deterministic)\n",
        "\n",
        "        # Dirs\n",
        "        self.save_dir = get_save_dir(self.args)\n",
        "        self.args.name = self.save_dir.name  # update name for loggers\n",
        "        self.wdir = self.save_dir / \"weights\"  # weights dir\n",
        "        if RANK in (-1, 0):\n",
        "            self.wdir.mkdir(parents=True, exist_ok=True)  # make dir\n",
        "            self.args.save_dir = str(self.save_dir)\n",
        "            yaml_save(self.save_dir / \"args.yaml\", vars(self.args))  # save run args\n",
        "        self.last, self.best = self.wdir / \"last.pt\", self.wdir / \"best.pt\"  # checkpoint paths\n",
        "        self.save_period = self.args.save_period\n",
        "\n",
        "        self.batch_size = self.args.batch\n",
        "        self.epochs = self.args.epochs\n",
        "        self.start_epoch = 0\n",
        "        if RANK == -1:\n",
        "            print_args(vars(self.args))\n",
        "\n",
        "        # Device\n",
        "        if self.device.type in (\"cpu\", \"mps\"):\n",
        "            self.args.workers = 0  # faster CPU training as time dominated by inference, not dataloading\n",
        "\n",
        "        # Model and Dataset\n",
        "        self.model = check_model_file_from_stem(self.args.model)  # add suffix, i.e. yolov8n -> yolov8n.pt\n",
        "        try:\n",
        "            if self.args.task == \"classify\":\n",
        "                self.data = check_cls_dataset(self.args.data)\n",
        "            elif self.args.data.split(\".\")[-1] in (\"yaml\", \"yml\") or self.args.task in (\"detect\", \"segment\", \"pose\"):\n",
        "                self.data = check_det_dataset(self.args.data)\n",
        "                if \"yaml_file\" in self.data:\n",
        "                    self.args.data = self.data[\"yaml_file\"]  # for validating 'yolo train data=url.zip' usage\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(emojis(f\"Dataset '{clean_url(self.args.data)}' error âŒ {e}\")) from e\n",
        "\n",
        "        self.trainset, self.testset = self.get_dataset(self.data)\n",
        "        self.ema = None\n",
        "\n",
        "        # Optimization utils init\n",
        "        self.lf = None\n",
        "        self.scheduler = None\n",
        "\n",
        "        # Epoch level metrics\n",
        "        self.best_fitness = None\n",
        "        self.fitness = None\n",
        "        self.loss = None\n",
        "        self.tloss = None\n",
        "        self.loss_names = [\"Loss\"]\n",
        "        self.csv = self.save_dir / \"results.csv\"\n",
        "        self.plot_idx = [0, 1, 2]\n",
        "\n",
        "        # Callbacks\n",
        "        self.callbacks = _callbacks or callbacks.get_default_callbacks()\n",
        "        if RANK in (-1, 0):\n",
        "            callbacks.add_integration_callbacks(self)\n",
        "\n",
        "    def add_callback(self, event: str, callback):\n",
        "        \"\"\"Appends the given callback.\"\"\"\n",
        "        self.callbacks[event].append(callback)\n",
        "\n",
        "    def set_callback(self, event: str, callback):\n",
        "        \"\"\"Overrides the existing callbacks with the given callback.\"\"\"\n",
        "        self.callbacks[event] = [callback]\n",
        "\n",
        "    def run_callbacks(self, event: str):\n",
        "        \"\"\"Run all existing callbacks associated with a particular event.\"\"\"\n",
        "        for callback in self.callbacks.get(event, []):\n",
        "            callback(self)\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Allow device='', device=None on Multi-GPU systems to default to device=0.\"\"\"\n",
        "        if isinstance(self.args.device, str) and len(self.args.device):  # i.e. device='0' or device='0,1,2,3'\n",
        "            world_size = len(self.args.device.split(\",\"))\n",
        "        elif isinstance(self.args.device, (tuple, list)):  # i.e. device=[0, 1, 2, 3] (multi-GPU from CLI is list)\n",
        "            world_size = len(self.args.device)\n",
        "        elif torch.cuda.is_available():  # i.e. device=None or device='' or device=number\n",
        "            world_size = 1  # default to device 0\n",
        "        else:  # i.e. device='cpu' or 'mps'\n",
        "            world_size = 0\n",
        "\n",
        "        # Run subprocess if DDP training, else train normally\n",
        "        if world_size > 1 and \"LOCAL_RANK\" not in os.environ:\n",
        "            # Argument checks\n",
        "            if self.args.rect:\n",
        "                LOGGER.warning(\"WARNING âš ï¸ 'rect=True' is incompatible with Multi-GPU training, setting 'rect=False'\")\n",
        "                self.args.rect = False\n",
        "            if self.args.batch == -1:\n",
        "                LOGGER.warning(\n",
        "                    \"WARNING âš ï¸ 'batch=-1' for AutoBatch is incompatible with Multi-GPU training, setting \"\n",
        "                    \"default 'batch=16'\"\n",
        "                )\n",
        "                self.args.batch = 16\n",
        "\n",
        "            # Command\n",
        "            cmd, file = generate_ddp_command(world_size, self)\n",
        "            try:\n",
        "                LOGGER.info(f'{colorstr(\"DDP:\")} debug command {\" \".join(cmd)}')\n",
        "                subprocess.run(cmd, check=True)\n",
        "            except Exception as e:\n",
        "                raise e\n",
        "            finally:\n",
        "                ddp_cleanup(self, str(file))\n",
        "\n",
        "        else:\n",
        "            self._do_train(world_size)\n",
        "\n",
        "    def _setup_scheduler(self):\n",
        "        \"\"\"Initialize training learning rate scheduler.\"\"\"\n",
        "        if self.args.cos_lr:\n",
        "            self.lf = one_cycle(1, self.args.lrf, self.epochs)  # cosine 1->hyp['lrf']\n",
        "        else:\n",
        "            self.lf = lambda x: max(1 - x / self.epochs, 0) * (1.0 - self.args.lrf) + self.args.lrf  # linear\n",
        "        self.scheduler = optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=self.lf)\n",
        "\n",
        "    def _setup_ddp(self, world_size):\n",
        "        \"\"\"Initializes and sets the DistributedDataParallel parameters for training.\"\"\"\n",
        "        torch.cuda.set_device(RANK)\n",
        "        self.device = torch.device(\"cuda\", RANK)\n",
        "        # LOGGER.info(f'DDP info: RANK {RANK}, WORLD_SIZE {world_size}, DEVICE {self.device}')\n",
        "        os.environ[\"NCCL_BLOCKING_WAIT\"] = \"1\"  # set to enforce timeout\n",
        "        dist.init_process_group(\n",
        "            \"nccl\" if dist.is_nccl_available() else \"gloo\",\n",
        "            timeout=timedelta(seconds=10800),  # 3 hours\n",
        "            rank=RANK,\n",
        "            world_size=world_size,\n",
        "        )\n",
        "\n",
        "    def _setup_train(self, world_size):\n",
        "        \"\"\"Builds dataloaders and optimizer on correct rank process.\"\"\"\n",
        "\n",
        "        # Model\n",
        "        self.run_callbacks(\"on_pretrain_routine_start\")\n",
        "        ckpt = self.setup_model()\n",
        "        self.model = self.model.to(self.device)\n",
        "        self.set_model_attributes()\n",
        "\n",
        "        # Freeze layers\n",
        "        freeze_list = (\n",
        "            self.args.freeze\n",
        "            if isinstance(self.args.freeze, list)\n",
        "            else range(self.args.freeze)\n",
        "            if isinstance(self.args.freeze, int)\n",
        "            else []\n",
        "        )\n",
        "        always_freeze_names = [\".dfl\"]  # always freeze these layers\n",
        "        freeze_layer_names = [f\"model.{x}.\" for x in freeze_list] + always_freeze_names\n",
        "        for k, v in self.model.named_parameters():\n",
        "            # v.register_hook(lambda x: torch.nan_to_num(x))  # NaN to 0 (commented for erratic training results)\n",
        "            if any(x in k for x in freeze_layer_names):\n",
        "                LOGGER.info(f\"Freezing layer '{k}'\")\n",
        "                v.requires_grad = False\n",
        "            elif not v.requires_grad and v.dtype.is_floating_point:  # only floating point Tensor can require gradients\n",
        "                LOGGER.info(\n",
        "                    f\"WARNING âš ï¸ setting 'requires_grad=True' for frozen layer '{k}'. \"\n",
        "                    \"See ultralytics.engine.trainer for customization of frozen layers.\"\n",
        "                )\n",
        "                v.requires_grad = True\n",
        "\n",
        "        # Check AMP\n",
        "        self.amp = torch.tensor(self.args.amp).to(self.device)  # True or False\n",
        "        if self.amp and RANK in (-1, 0):  # Single-GPU and DDP\n",
        "            callbacks_backup = callbacks.default_callbacks.copy()  # backup callbacks as check_amp() resets them\n",
        "            self.amp = torch.tensor(check_amp(self.model), device=self.device)\n",
        "            callbacks.default_callbacks = callbacks_backup  # restore callbacks\n",
        "        if RANK > -1 and world_size > 1:  # DDP\n",
        "            dist.broadcast(self.amp, src=0)  # broadcast the tensor from rank 0 to all other ranks (returns None)\n",
        "        self.amp = bool(self.amp)  # as boolean\n",
        "        self.scaler = torch.cuda.amp.GradScaler(enabled=self.amp)\n",
        "        if world_size > 1:\n",
        "            self.model = nn.parallel.DistributedDataParallel(self.model, device_ids=[RANK])\n",
        "\n",
        "        # Check imgsz\n",
        "        gs = max(int(self.model.stride.max() if hasattr(self.model, \"stride\") else 32), 32)  # grid size (max stride)\n",
        "        self.args.imgsz = check_imgsz(self.args.imgsz, stride=gs, floor=gs, max_dim=1)\n",
        "        self.stride = gs  # for multi-scale training\n",
        "\n",
        "        # Batch size\n",
        "        if self.batch_size == -1 and RANK == -1:  # single-GPU only, estimate best batch size\n",
        "            self.args.batch = self.batch_size = check_train_batch_size(self.model, self.args.imgsz, self.amp)\n",
        "\n",
        "        # Dataloaders\n",
        "        batch_size = self.batch_size // max(world_size, 1)\n",
        "        self.train_loader = self.get_dataloader(self.trainset, batch_size=batch_size, rank=RANK, mode=\"train\")\n",
        "        if RANK in (-1, 0):\n",
        "            # Note: When training DOTA dataset, double batch size could get OOM on images with >2000 objects.\n",
        "            self.test_loader = self.get_dataloader(\n",
        "                self.testset, batch_size=batch_size if self.args.task == \"obb\" else batch_size * 2, rank=-1, mode=\"val\"\n",
        "            )\n",
        "            self.validator = self.get_validator()\n",
        "            metric_keys = self.validator.metrics.keys + self.label_loss_items(prefix=\"val\")\n",
        "            self.metrics = dict(zip(metric_keys, [0] * len(metric_keys)))\n",
        "            self.ema = ModelEMA(self.model)\n",
        "            if self.args.plots:\n",
        "                self.plot_training_labels()\n",
        "\n",
        "        # -------------------------------------\n",
        "        self.model.stu_feature_adapt1 = nn.Sequential(nn.Conv2d(64, 192, 3, padding=1, stride=1), nn.ReLU()).to(self.device)\n",
        "        self.model.stu_feature_adapt2 = nn.Sequential(nn.Conv2d(128, 384, 3, padding=1, stride=1), nn.ReLU()).to(self.device)\n",
        "        self.model.stu_feature_adapt3 = nn.Sequential(nn.Conv2d(256, 576, 3, padding=1, stride=1), nn.ReLU()).to(self.device)\n",
        "\n",
        "        self.model.project_head = nn.Sequential(\n",
        "                                        nn.AdaptiveAvgPool2d((1, 1)),\n",
        "                                        nn.Flatten(),\n",
        "                                        nn.Linear(256, 128),\n",
        "                                        nn.ReLU(),\n",
        "                                        nn.Linear(128, 64),\n",
        "                                        nn.Tanh(),\n",
        "                                        nn.Linear(64, 3),\n",
        "                                        nn.Softmax()\n",
        "                                    )\n",
        "        # Optimizer\n",
        "        self.accumulate = max(round(self.args.nbs / self.batch_size), 1)  # accumulate loss before optimizing\n",
        "        weight_decay = self.args.weight_decay * self.batch_size * self.accumulate / self.args.nbs  # scale weight_decay\n",
        "        iterations = math.ceil(len(self.train_loader.dataset) / max(self.batch_size, self.args.nbs)) * self.epochs\n",
        "        self.optimizer = self.build_optimizer(\n",
        "            model=self.model,\n",
        "            name=self.args.optimizer,\n",
        "            lr=self.args.lr0,\n",
        "            momentum=self.args.momentum,\n",
        "            decay=weight_decay,\n",
        "            iterations=iterations,\n",
        "        )\n",
        "        # Scheduler\n",
        "        self._setup_scheduler()\n",
        "        self.stopper, self.stop = EarlyStopping(patience=self.args.patience), False\n",
        "        self.resume_training(ckpt)\n",
        "        self.scheduler.last_epoch = self.start_epoch - 1  # do not move\n",
        "        self.run_callbacks(\"on_pretrain_routine_end\")\n",
        "\n",
        "    def _do_train(self, world_size=1):\n",
        "        \"\"\"Train completed, evaluate and plot if specified by arguments.\"\"\"\n",
        "        if world_size > 1:\n",
        "            self._setup_ddp(world_size)\n",
        "        self._setup_train(world_size)\n",
        "\n",
        "        nb = len(self.train_loader)  # number of batches\n",
        "        nw = max(round(self.args.warmup_epochs * nb), 100) if self.args.warmup_epochs > 0 else -1  # warmup iterations\n",
        "        last_opt_step = -1\n",
        "        self.epoch_time = None\n",
        "        self.epoch_time_start = time.time()\n",
        "        self.train_time_start = time.time()\n",
        "        self.run_callbacks(\"on_train_start\")\n",
        "        LOGGER.info(\n",
        "            f'Image sizes {self.args.imgsz} train, {self.args.imgsz} val\\n'\n",
        "            f'Using {self.train_loader.num_workers * (world_size or 1)} dataloader workers\\n'\n",
        "            f\"Logging results to {colorstr('bold', self.save_dir)}\\n\"\n",
        "            f'Starting training for ' + (f\"{self.args.time} hours...\" if self.args.time else f\"{self.epochs} epochs...\")\n",
        "        )\n",
        "        if self.args.close_mosaic:\n",
        "            base_idx = (self.epochs - self.args.close_mosaic) * nb\n",
        "            self.plot_idx.extend([base_idx, base_idx + 1, base_idx + 2])\n",
        "        epoch = self.start_epoch\n",
        "\n",
        "        # -----add hook\n",
        "        from ultralytics import YOLO\n",
        "\n",
        "        # model = YOLO('yolov8m.pt')\n",
        "\n",
        "        activation_student = {}\n",
        "        activation_teacher = {}\n",
        "        def get_activation_st(name):\n",
        "            def hook(model, input, output):\n",
        "                activation_student[name] = output\n",
        "            return hook\n",
        "        def get_activation_tc(name):\n",
        "            def hook(model, input, output):\n",
        "                activation_teacher[name] = output\n",
        "            return hook\n",
        "\n",
        "        # self.model\n",
        "        teacher1 = YOLO('/content/last.pt').model\n",
        "        teacher1 = teacher1.half()\n",
        "        teacher1 = teacher1.to(self.device)\n",
        "\n",
        "        teacher2 = YOLOGhost('/content/lastGhost.pt').model\n",
        "        teacher2 = teacher2.half()\n",
        "        teacher2 = teacher2.to(self.device)\n",
        "\n",
        "        teacher3 = YOLOTransformer('/content/lastTransformer.pt').model\n",
        "        teacher3 = teacher3.half()\n",
        "        teacher3 = teacher3.to(self.device)\n",
        "        print(self.device)\n",
        "\n",
        "\n",
        "        try:\n",
        "            for name, param in teacher.named_parameters():\n",
        "                print(f\"Parameter: {name}, Type: {param.dtype}, Size: {param.size()}\")\n",
        "                assert False\n",
        "        except:\n",
        "            pass\n",
        "        teacher.eval()\n",
        "\n",
        "        self.model.model[3].register_forward_hook(get_activation_st('feat3'))\n",
        "        self.model.model[5].register_forward_hook(get_activation_st('feat5'))\n",
        "        self.model.model[7].register_forward_hook(get_activation_st('feat7'))\n",
        "        self.model.model[9].register_forward_hook(get_activation_st('feat7'))\n",
        "\n",
        "        teacher1.model[3].register_forward_hook(get_activation_tc('1_feat3'))\n",
        "        teacher1.model[5].register_forward_hook(get_activation_tc('1_feat5'))\n",
        "        teacher1.model[7].register_forward_hook(get_activation_tc('1_feat7'))\n",
        "\n",
        "        teacher2.model[3].register_forward_hook(get_activation_tc('2_feat3'))\n",
        "        teacher2.model[5].register_forward_hook(get_activation_tc('2_feat5'))\n",
        "        teacher2.model[7].register_forward_hook(get_activation_tc('2_feat7'))\n",
        "\n",
        "        teacher3.model[3].register_forward_hook(get_activation_tc('3_feat3'))\n",
        "        teacher3.model[5].register_forward_hook(get_activation_tc('3_feat5'))\n",
        "        teacher3.model[7].register_forward_hook(get_activation_tc('3_feat7'))\n",
        "\n",
        "        print(\"OK\"*100)\n",
        "        # ------------------------------------------\n",
        "\n",
        "\n",
        "        while True:\n",
        "            self.epoch = epoch\n",
        "            self.run_callbacks(\"on_train_epoch_start\")\n",
        "            self.model.train()\n",
        "            if RANK != -1:\n",
        "                self.train_loader.sampler.set_epoch(epoch)\n",
        "            pbar = enumerate(self.train_loader)\n",
        "            # Update dataloader attributes (optional)\n",
        "            if epoch == (self.epochs - self.args.close_mosaic):\n",
        "                self._close_dataloader_mosaic()\n",
        "                self.train_loader.reset()\n",
        "\n",
        "            if RANK in (-1, 0):\n",
        "                LOGGER.info(self.progress_string())\n",
        "                pbar = TQDM(enumerate(self.train_loader), total=nb)\n",
        "            self.tloss = None\n",
        "            self.optimizer.zero_grad()\n",
        "            for i, batch in pbar:\n",
        "                self.run_callbacks(\"on_train_batch_start\")\n",
        "                # Warmup\n",
        "                ni = i + nb * epoch\n",
        "                if ni <= nw:\n",
        "                    xi = [0, nw]  # x interp\n",
        "                    self.accumulate = max(1, int(np.interp(ni, xi, [1, self.args.nbs / self.batch_size]).round()))\n",
        "                    for j, x in enumerate(self.optimizer.param_groups):\n",
        "                        # Bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0\n",
        "                        x[\"lr\"] = np.interp(\n",
        "                            ni, xi, [self.args.warmup_bias_lr if j == 0 else 0.0, x[\"initial_lr\"] * self.lf(epoch)]\n",
        "                        )\n",
        "                        if \"momentum\" in x:\n",
        "                            x[\"momentum\"] = np.interp(ni, xi, [self.args.warmup_momentum, self.args.momentum])\n",
        "\n",
        "                # Forward\n",
        "                with torch.cuda.amp.autocast(self.amp):\n",
        "                    batch = self.preprocess_batch(batch)\n",
        "#                     print(batch.keys())\n",
        "                    self.loss, self.loss_items = self.model(batch)\n",
        "                    with torch.no_grad():\n",
        "                        teacher(batch[\"img\"])\n",
        "\n",
        "                    adaptive = self.model.project_head(activation_student['feat9'])\n",
        "\n",
        "#                     loss_kd = torch.tensor(0).to(device)\n",
        "                    self.loss += (F.mse_loss(\n",
        "                        self.model.stu_feature_adapt1(activation_student['feat3']),\n",
        "                        activation_teacher['1_feat3'],\n",
        "                        reduction='none'\n",
        "                    ) * adaptive[:,0]).mean()\n",
        "                    self.loss += (F.mse_loss(\n",
        "                        self.model.stu_feature_adapt2(activation_student['feat5']),\n",
        "                        activation_teacher['1_feat5'],\n",
        "                        reduction='none'\n",
        "                    ) * adaptive[:,0]).mean()\n",
        "                    self.loss += (F.mse_loss(\n",
        "                        self.model.stu_feature_adapt3(activation_student['feat7']),\n",
        "                        activation_teacher['1_feat7'],\n",
        "                        reduction='none'\n",
        "                    ) * adaptive[:,0]).mean()\n",
        "\n",
        "                    self.loss += (F.mse_loss(\n",
        "                        self.model.stu_feature_adapt1(activation_student['feat3']),\n",
        "                        activation_teacher['2_feat3'],\n",
        "                        reduction='none'\n",
        "                    ) * adaptive[:,1]).mean()\n",
        "                    self.loss += (F.mse_loss(\n",
        "                        self.model.stu_feature_adapt2(activation_student['feat5']),\n",
        "                        activation_teacher['2_feat5'],\n",
        "                        reduction='none'\n",
        "                    ) * adaptive[:,1]).mean()\n",
        "                    self.loss += (F.mse_loss(\n",
        "                        self.model.stu_feature_adapt3(activation_student['feat7']),\n",
        "                        activation_teacher['2_feat7'],\n",
        "                        reduction='none'\n",
        "                    ) * adaptive[:,1]).mean()\n",
        "\n",
        "                    self.loss += (F.mse_loss(\n",
        "                        self.model.stu_feature_adapt1(activation_student['feat3']),\n",
        "                        activation_teacher['3_feat3'],\n",
        "                        reduction='none'\n",
        "                    ) * adaptive[:,2]).mean()\n",
        "                    self.loss += (F.mse_loss(\n",
        "                        self.model.stu_feature_adapt2(activation_student['feat5']),\n",
        "                        activation_teacher['3_feat5'],\n",
        "                        reduction='none'\n",
        "                    ) * adaptive[:,2]).mean()\n",
        "                    self.loss += (F.mse_loss(\n",
        "                        self.model.stu_feature_adapt3(activation_student['feat7']),\n",
        "                        activation_teacher['3_feat7'],\n",
        "                        reduction='none'\n",
        "                    ) * adaptive[:,2]).mean()\n",
        "                        # activation_student['feat3']\n",
        "#                     self.loss += loss_kd\n",
        "\n",
        "                    if RANK != -1:\n",
        "                        self.loss *= world_size\n",
        "                    self.tloss = (\n",
        "                        (self.tloss * i + self.loss_items) / (i + 1) if self.tloss is not None else self.loss_items\n",
        "                    )\n",
        "\n",
        "                # Backward\n",
        "                self.scaler.scale(self.loss).backward()\n",
        "\n",
        "                # Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\n",
        "                if ni - last_opt_step >= self.accumulate:\n",
        "                    self.optimizer_step()\n",
        "                    last_opt_step = ni\n",
        "\n",
        "                    # Timed stopping\n",
        "                    if self.args.time:\n",
        "                        self.stop = (time.time() - self.train_time_start) > (self.args.time * 3600)\n",
        "                        if RANK != -1:  # if DDP training\n",
        "                            broadcast_list = [self.stop if RANK == 0 else None]\n",
        "                            dist.broadcast_object_list(broadcast_list, 0)  # broadcast 'stop' to all ranks\n",
        "                            self.stop = broadcast_list[0]\n",
        "                        if self.stop:  # training time exceeded\n",
        "                            break\n",
        "\n",
        "                # Log\n",
        "                mem = f\"{torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0:.3g}G\"  # (GB)\n",
        "                loss_len = self.tloss.shape[0] if len(self.tloss.shape) else 1\n",
        "                losses = self.tloss if loss_len > 1 else torch.unsqueeze(self.tloss, 0)\n",
        "                if RANK in (-1, 0):\n",
        "                    pbar.set_description(\n",
        "                        (\"%11s\" * 2 + \"%11.4g\" * (2 + loss_len))\n",
        "                        % (f\"{epoch + 1}/{self.epochs}\", mem, *losses, batch[\"cls\"].shape[0], batch[\"img\"].shape[-1])\n",
        "                    )\n",
        "                    self.run_callbacks(\"on_batch_end\")\n",
        "                    if self.args.plots and ni in self.plot_idx:\n",
        "                        self.plot_training_samples(batch, ni)\n",
        "\n",
        "                self.run_callbacks(\"on_train_batch_end\")\n",
        "\n",
        "            self.lr = {f\"lr/pg{ir}\": x[\"lr\"] for ir, x in enumerate(self.optimizer.param_groups)}  # for loggers\n",
        "            self.run_callbacks(\"on_train_epoch_end\")\n",
        "            if RANK in (-1, 0):\n",
        "                final_epoch = epoch + 1 == self.epochs\n",
        "                self.ema.update_attr(self.model, include=[\"yaml\", \"nc\", \"args\", \"names\", \"stride\", \"class_weights\"])\n",
        "\n",
        "                # Validation\n",
        "                if self.args.val or final_epoch or self.stopper.possible_stop or self.stop:\n",
        "                    self.metrics, self.fitness = self.validate()\n",
        "                self.save_metrics(metrics={**self.label_loss_items(self.tloss), **self.metrics, **self.lr})\n",
        "                self.stop |= self.stopper(epoch + 1, self.fitness) or final_epoch\n",
        "                if self.args.time:\n",
        "                    self.stop |= (time.time() - self.train_time_start) > (self.args.time * 3600)\n",
        "\n",
        "                # Save model\n",
        "                if self.args.save or final_epoch:\n",
        "                    self.save_model()\n",
        "                    self.run_callbacks(\"on_model_save\")\n",
        "\n",
        "            # Scheduler\n",
        "            t = time.time()\n",
        "            self.epoch_time = t - self.epoch_time_start\n",
        "            self.epoch_time_start = t\n",
        "            with warnings.catch_warnings():\n",
        "                warnings.simplefilter(\"ignore\")  # suppress 'Detected lr_scheduler.step() before optimizer.step()'\n",
        "                if self.args.time:\n",
        "                    mean_epoch_time = (t - self.train_time_start) / (epoch - self.start_epoch + 1)\n",
        "                    self.epochs = self.args.epochs = math.ceil(self.args.time * 3600 / mean_epoch_time)\n",
        "                    self._setup_scheduler()\n",
        "                    self.scheduler.last_epoch = self.epoch  # do not move\n",
        "                    self.stop |= epoch >= self.epochs  # stop if exceeded epochs\n",
        "                self.scheduler.step()\n",
        "            self.run_callbacks(\"on_fit_epoch_end\")\n",
        "            torch.cuda.empty_cache()  # clear GPU memory at end of epoch, may help reduce CUDA out of memory errors\n",
        "\n",
        "            # Early Stopping\n",
        "            if RANK != -1:  # if DDP training\n",
        "                broadcast_list = [self.stop if RANK == 0 else None]\n",
        "                dist.broadcast_object_list(broadcast_list, 0)  # broadcast 'stop' to all ranks\n",
        "                self.stop = broadcast_list[0]\n",
        "            if self.stop:\n",
        "                break  # must break all DDP ranks\n",
        "            epoch += 1\n",
        "\n",
        "        if RANK in (-1, 0):\n",
        "            # Do final val with best.pt\n",
        "            LOGGER.info(\n",
        "                f\"\\n{epoch - self.start_epoch + 1} epochs completed in \"\n",
        "                f\"{(time.time() - self.train_time_start) / 3600:.3f} hours.\"\n",
        "            )\n",
        "            self.final_eval()\n",
        "            if self.args.plots:\n",
        "                self.plot_metrics()\n",
        "            self.run_callbacks(\"on_train_end\")\n",
        "        torch.cuda.empty_cache()\n",
        "        self.run_callbacks(\"teardown\")\n",
        "\n",
        "    def save_model(self):\n",
        "        \"\"\"Save model training checkpoints with additional metadata.\"\"\"\n",
        "        import pandas as pd  # scope for faster startup\n",
        "\n",
        "        metrics = {**self.metrics, **{\"fitness\": self.fitness}}\n",
        "        results = {k.strip(): v for k, v in pd.read_csv(self.csv).to_dict(orient=\"list\").items()}\n",
        "        ckpt = {\n",
        "            \"epoch\": self.epoch,\n",
        "            \"best_fitness\": self.best_fitness,\n",
        "            \"model\": deepcopy(de_parallel(self.model)).half(),\n",
        "            \"ema\": deepcopy(self.ema.ema).half(),\n",
        "            \"updates\": self.ema.updates,\n",
        "            \"optimizer\": self.optimizer.state_dict(),\n",
        "            \"train_args\": vars(self.args),  # save as dict\n",
        "            \"train_metrics\": metrics,\n",
        "            \"train_results\": results,\n",
        "            \"date\": datetime.now().isoformat(),\n",
        "            \"version\": __version__,\n",
        "        }\n",
        "\n",
        "        # Save last and best\n",
        "        torch.save(ckpt, self.last)\n",
        "        if self.best_fitness == self.fitness:\n",
        "            torch.save(ckpt, self.best)\n",
        "        if (self.save_period > 0) and (self.epoch > 0) and (self.epoch % self.save_period == 0):\n",
        "            torch.save(ckpt, self.wdir / f\"epoch{self.epoch}.pt\")\n",
        "\n",
        "    @staticmethod\n",
        "    def get_dataset(data):\n",
        "        \"\"\"\n",
        "        Get train, val path from data dict if it exists.\n",
        "\n",
        "        Returns None if data format is not recognized.\n",
        "        \"\"\"\n",
        "        return data[\"train\"], data.get(\"val\") or data.get(\"test\")\n",
        "\n",
        "    def setup_model(self):\n",
        "        \"\"\"Load/create/download model for any task.\"\"\"\n",
        "        if isinstance(self.model, torch.nn.Module):  # if model is loaded beforehand. No setup needed\n",
        "            return\n",
        "\n",
        "        model, weights = self.model, None\n",
        "        ckpt = None\n",
        "        if str(model).endswith(\".pt\"):\n",
        "            weights, ckpt = attempt_load_one_weight(model)\n",
        "            cfg = ckpt[\"model\"].yaml\n",
        "        else:\n",
        "            cfg = model\n",
        "        self.model = self.get_model(cfg=cfg, weights=weights, verbose=RANK == -1)  # calls Model(cfg, weights)\n",
        "        return ckpt\n",
        "\n",
        "    def optimizer_step(self):\n",
        "        \"\"\"Perform a single step of the training optimizer with gradient clipping and EMA update.\"\"\"\n",
        "        self.scaler.unscale_(self.optimizer)  # unscale gradients\n",
        "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=10.0)  # clip gradients\n",
        "        self.scaler.step(self.optimizer)\n",
        "        self.scaler.update()\n",
        "        self.optimizer.zero_grad()\n",
        "        if self.ema:\n",
        "            self.ema.update(self.model)\n",
        "\n",
        "    def preprocess_batch(self, batch):\n",
        "        \"\"\"Allows custom preprocessing model inputs and ground truths depending on task type.\"\"\"\n",
        "        return batch\n",
        "\n",
        "    def validate(self):\n",
        "        \"\"\"\n",
        "        Runs validation on test set using self.validator.\n",
        "\n",
        "        The returned dict is expected to contain \"fitness\" key.\n",
        "        \"\"\"\n",
        "        metrics = self.validator(self)\n",
        "        fitness = metrics.pop(\"fitness\", -self.loss.detach().cpu().numpy())  # use loss as fitness measure if not found\n",
        "        if not self.best_fitness or self.best_fitness < fitness:\n",
        "            self.best_fitness = fitness\n",
        "        return metrics, fitness\n",
        "\n",
        "    def get_model(self, cfg=None, weights=None, verbose=True):\n",
        "        \"\"\"Get model and raise NotImplementedError for loading cfg files.\"\"\"\n",
        "        raise NotImplementedError(\"This task trainer doesn't support loading cfg files\")\n",
        "\n",
        "    def get_validator(self):\n",
        "        \"\"\"Returns a NotImplementedError when the get_validator function is called.\"\"\"\n",
        "        raise NotImplementedError(\"get_validator function not implemented in trainer\")\n",
        "\n",
        "    def get_dataloader(self, dataset_path, batch_size=16, rank=0, mode=\"train\"):\n",
        "        \"\"\"Returns dataloader derived from torch.data.Dataloader.\"\"\"\n",
        "        raise NotImplementedError(\"get_dataloader function not implemented in trainer\")\n",
        "\n",
        "    def build_dataset(self, img_path, mode=\"train\", batch=None):\n",
        "        \"\"\"Build dataset.\"\"\"\n",
        "        raise NotImplementedError(\"build_dataset function not implemented in trainer\")\n",
        "\n",
        "    def label_loss_items(self, loss_items=None, prefix=\"train\"):\n",
        "        \"\"\"\n",
        "        Returns a loss dict with labelled training loss items tensor.\n",
        "\n",
        "        Note:\n",
        "            This is not needed for classification but necessary for segmentation & detection\n",
        "        \"\"\"\n",
        "        return {\"loss\": loss_items} if loss_items is not None else [\"loss\"]\n",
        "\n",
        "    def set_model_attributes(self):\n",
        "        \"\"\"To set or update model parameters before training.\"\"\"\n",
        "        self.model.names = self.data[\"names\"]\n",
        "\n",
        "    def build_targets(self, preds, targets):\n",
        "        \"\"\"Builds target tensors for training YOLO model.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def progress_string(self):\n",
        "        \"\"\"Returns a string describing training progress.\"\"\"\n",
        "        return \"\"\n",
        "\n",
        "    # TODO: may need to put these following functions into callback\n",
        "    def plot_training_samples(self, batch, ni):\n",
        "        \"\"\"Plots training samples during YOLO training.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def plot_training_labels(self):\n",
        "        \"\"\"Plots training labels for YOLO model.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def save_metrics(self, metrics):\n",
        "        \"\"\"Saves training metrics to a CSV file.\"\"\"\n",
        "        keys, vals = list(metrics.keys()), list(metrics.values())\n",
        "        n = len(metrics) + 1  # number of cols\n",
        "        s = \"\" if self.csv.exists() else ((\"%23s,\" * n % tuple([\"epoch\"] + keys)).rstrip(\",\") + \"\\n\")  # header\n",
        "        with open(self.csv, \"a\") as f:\n",
        "            f.write(s + (\"%23.5g,\" * n % tuple([self.epoch + 1] + vals)).rstrip(\",\") + \"\\n\")\n",
        "\n",
        "    def plot_metrics(self):\n",
        "        \"\"\"Plot and display metrics visually.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def on_plot(self, name, data=None):\n",
        "        \"\"\"Registers plots (e.g. to be consumed in callbacks)\"\"\"\n",
        "        path = Path(name)\n",
        "        self.plots[path] = {\"data\": data, \"timestamp\": time.time()}\n",
        "\n",
        "    def final_eval(self):\n",
        "        \"\"\"Performs final evaluation and validation for object detection YOLO model.\"\"\"\n",
        "        for f in self.last, self.best:\n",
        "            if f.exists():\n",
        "                strip_optimizer(f)  # strip optimizers\n",
        "                if f is self.best:\n",
        "                    LOGGER.info(f\"\\nValidating {f}...\")\n",
        "                    self.validator.args.plots = self.args.plots\n",
        "                    self.metrics = self.validator(model=f)\n",
        "                    self.metrics.pop(\"fitness\", None)\n",
        "                    self.run_callbacks(\"on_fit_epoch_end\")\n",
        "\n",
        "    def check_resume(self, overrides):\n",
        "        \"\"\"Check if resume checkpoint exists and update arguments accordingly.\"\"\"\n",
        "        resume = self.args.resume\n",
        "        if resume:\n",
        "            try:\n",
        "                exists = isinstance(resume, (str, Path)) and Path(resume).exists()\n",
        "                last = Path(check_file(resume) if exists else get_latest_run())\n",
        "\n",
        "                # Check that resume data YAML exists, otherwise strip to force re-download of dataset\n",
        "                ckpt_args = attempt_load_weights(last).args\n",
        "                if not Path(ckpt_args[\"data\"]).exists():\n",
        "                    ckpt_args[\"data\"] = self.args.data\n",
        "\n",
        "                resume = True\n",
        "                self.args = get_cfg(ckpt_args)\n",
        "                self.args.model = str(last)  # reinstate model\n",
        "                for k in \"imgsz\", \"batch\":  # allow arg updates to reduce memory on resume if crashed due to CUDA OOM\n",
        "                    if k in overrides:\n",
        "                        setattr(self.args, k, overrides[k])\n",
        "\n",
        "            except Exception as e:\n",
        "                raise FileNotFoundError(\n",
        "                    \"Resume checkpoint not found. Please pass a valid checkpoint to resume from, \"\n",
        "                    \"i.e. 'yolo train resume model=path/to/last.pt'\"\n",
        "                ) from e\n",
        "        self.resume = resume\n",
        "\n",
        "    def resume_training(self, ckpt):\n",
        "        \"\"\"Resume YOLO training from given epoch and best fitness.\"\"\"\n",
        "        if ckpt is None:\n",
        "            return\n",
        "        best_fitness = 0.0\n",
        "        start_epoch = ckpt[\"epoch\"] + 1\n",
        "        if ckpt[\"optimizer\"] is not None:\n",
        "            self.optimizer.load_state_dict(ckpt[\"optimizer\"])  # optimizer\n",
        "            best_fitness = ckpt[\"best_fitness\"]\n",
        "        if self.ema and ckpt.get(\"ema\"):\n",
        "            self.ema.ema.load_state_dict(ckpt[\"ema\"].float().state_dict())  # EMA\n",
        "            self.ema.updates = ckpt[\"updates\"]\n",
        "        if self.resume:\n",
        "            assert start_epoch > 0, (\n",
        "                f\"{self.args.model} training to {self.epochs} epochs is finished, nothing to resume.\\n\"\n",
        "                f\"Start a new training without resuming, i.e. 'yolo train model={self.args.model}'\"\n",
        "            )\n",
        "            LOGGER.info(\n",
        "                f\"Resuming training from {self.args.model} from epoch {start_epoch + 1} to {self.epochs} total epochs\"\n",
        "            )\n",
        "        if self.epochs < start_epoch:\n",
        "            LOGGER.info(\n",
        "                f\"{self.model} has been trained for {ckpt['epoch']} epochs. Fine-tuning for {self.epochs} more epochs.\"\n",
        "            )\n",
        "            self.epochs += ckpt[\"epoch\"]  # finetune additional epochs\n",
        "        self.best_fitness = best_fitness\n",
        "        self.start_epoch = start_epoch\n",
        "        if start_epoch > (self.epochs - self.args.close_mosaic):\n",
        "            self._close_dataloader_mosaic()\n",
        "\n",
        "    def _close_dataloader_mosaic(self):\n",
        "        \"\"\"Update dataloaders to stop using mosaic augmentation.\"\"\"\n",
        "        if hasattr(self.train_loader.dataset, \"mosaic\"):\n",
        "            self.train_loader.dataset.mosaic = False\n",
        "        if hasattr(self.train_loader.dataset, \"close_mosaic\"):\n",
        "            LOGGER.info(\"Closing dataloader mosaic\")\n",
        "            self.train_loader.dataset.close_mosaic(hyp=self.args)\n",
        "\n",
        "    def build_optimizer(self, model, name=\"auto\", lr=0.001, momentum=0.9, decay=1e-5, iterations=1e5):\n",
        "        \"\"\"\n",
        "        Constructs an optimizer for the given model, based on the specified optimizer name, learning rate, momentum,\n",
        "        weight decay, and number of iterations.\n",
        "\n",
        "        Args:\n",
        "            model (torch.nn.Module): The model for which to build an optimizer.\n",
        "            name (str, optional): The name of the optimizer to use. If 'auto', the optimizer is selected\n",
        "                based on the number of iterations. Default: 'auto'.\n",
        "            lr (float, optional): The learning rate for the optimizer. Default: 0.001.\n",
        "            momentum (float, optional): The momentum factor for the optimizer. Default: 0.9.\n",
        "            decay (float, optional): The weight decay for the optimizer. Default: 1e-5.\n",
        "            iterations (float, optional): The number of iterations, which determines the optimizer if\n",
        "                name is 'auto'. Default: 1e5.\n",
        "\n",
        "        Returns:\n",
        "            (torch.optim.Optimizer): The constructed optimizer.\n",
        "        \"\"\"\n",
        "\n",
        "        g = [], [], []  # optimizer parameter groups\n",
        "        bn = tuple(v for k, v in nn.__dict__.items() if \"Norm\" in k)  # normalization layers, i.e. BatchNorm2d()\n",
        "        if name == \"auto\":\n",
        "            LOGGER.info(\n",
        "                f\"{colorstr('optimizer:')} 'optimizer=auto' found, \"\n",
        "                f\"ignoring 'lr0={self.args.lr0}' and 'momentum={self.args.momentum}' and \"\n",
        "                f\"determining best 'optimizer', 'lr0' and 'momentum' automatically... \"\n",
        "            )\n",
        "            nc = getattr(model, \"nc\", 10)  # number of classes\n",
        "            lr_fit = round(0.002 * 5 / (4 + nc), 6)  # lr0 fit equation to 6 decimal places\n",
        "            name, lr, momentum = (\"SGD\", 0.01, 0.9) if iterations > 10000 else (\"AdamW\", lr_fit, 0.9)\n",
        "            self.args.warmup_bias_lr = 0.0  # no higher than 0.01 for Adam\n",
        "\n",
        "        for module_name, module in model.named_modules():\n",
        "            for param_name, param in module.named_parameters(recurse=False):\n",
        "                fullname = f\"{module_name}.{param_name}\" if module_name else param_name\n",
        "                if \"bias\" in fullname:  # bias (no decay)\n",
        "                    g[2].append(param)\n",
        "                elif isinstance(module, bn):  # weight (no decay)\n",
        "                    g[1].append(param)\n",
        "                else:  # weight (with decay)\n",
        "                    g[0].append(param)\n",
        "\n",
        "        if name in (\"Adam\", \"Adamax\", \"AdamW\", \"NAdam\", \"RAdam\"):\n",
        "            optimizer = getattr(optim, name, optim.Adam)(g[2], lr=lr, betas=(momentum, 0.999), weight_decay=0.0)\n",
        "        elif name == \"RMSProp\":\n",
        "            optimizer = optim.RMSprop(g[2], lr=lr, momentum=momentum)\n",
        "        elif name == \"SGD\":\n",
        "            optimizer = optim.SGD(g[2], lr=lr, momentum=momentum, nesterov=True)\n",
        "        else:\n",
        "            raise NotImplementedError(\n",
        "                f\"Optimizer '{name}' not found in list of available optimizers \"\n",
        "                f\"[Adam, AdamW, NAdam, RAdam, RMSProp, SGD, auto].\"\n",
        "                \"To request support for addition optimizers please visit https://github.com/ultralytics/ultralytics.\"\n",
        "            )\n",
        "\n",
        "        optimizer.add_param_group({\"params\": g[0], \"weight_decay\": decay})  # add g0 with weight_decay\n",
        "        optimizer.add_param_group({\"params\": g[1], \"weight_decay\": 0.0})  # add g1 (BatchNorm2d weights)\n",
        "        LOGGER.info(\n",
        "            f\"{colorstr('optimizer:')} {type(optimizer).__name__}(lr={lr}, momentum={momentum}) with parameter groups \"\n",
        "            f'{len(g[1])} weight(decay=0.0), {len(g[0])} weight(decay={decay}), {len(g[2])} bias(decay=0.0)'\n",
        "        )\n",
        "        return optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Isqb0_tyllcb"
      },
      "outputs": [],
      "source": [
        "%%writefile my_yolov8.yaml\n",
        "# Ultralytics YOLO ðŸš€, AGPL-3.0 license\n",
        "# YOLOv8 object detection model with P3-P5 outputs. For Usage examples see https://docs.ultralytics.com/tasks/detect\n",
        "\n",
        "# Parameters\n",
        "nc: 80 # number of classes\n",
        "scales: # model compound scaling constants, i.e. 'model=yolov8n.yaml' will call yolov8.yaml with scale 'n'\n",
        "  # [depth, width, max_channels]\n",
        "  n: [0.165, 0.25, 1024] # YOLOv8n summary: 225 layers,  3157200 parameters,  3157184 gradients,   8.9 GFLOPs\n",
        "  s: [0.33, 0.50, 1024] # YOLOv8s summary: 225 layers, 11166560 parameters, 11166544 gradients,  28.8 GFLOPs\n",
        "  m: [0.67, 0.75, 768] # YOLOv8m summary: 295 layers, 25902640 parameters, 25902624 gradients,  79.3 GFLOPs\n",
        "  l: [1.00, 1.00, 512] # YOLOv8l summary: 365 layers, 43691520 parameters, 43691504 gradients, 165.7 GFLOPs\n",
        "  x: [1.00, 1.25, 512] # YOLOv8x summary: 365 layers, 68229648 parameters, 68229632 gradients, 258.5 GFLOPs\n",
        "\n",
        "# YOLOv8.0n backbone\n",
        "backbone:\n",
        "  # [from, repeats, module, args]\n",
        "  - [-1, 1, Conv, [64, 3, 2]] # 0-P1/2\n",
        "  - [-1, 1, Conv, [128, 3, 2]] # 1-P2/4\n",
        "  - [-1, 3, C2f, [128, True]]\n",
        "  - [-1, 1, Conv, [256, 3, 2]] # 3-P3/8\n",
        "  - [-1, 6, C2f, [256, True]]\n",
        "  - [-1, 1, Conv, [512, 3, 2]] # 5-P4/16\n",
        "  - [-1, 6, C2f, [512, True]]\n",
        "  - [-1, 1, Conv, [1024, 3, 2]] # 7-P5/32\n",
        "  - [-1, 3, C2f, [1024, True]]\n",
        "  - [-1, 1, SPPF, [1024, 5]] # 9\n",
        "\n",
        "# YOLOv8.0n head\n",
        "head:\n",
        "  - [-1, 1, nn.Upsample, [None, 2, \"nearest\"]]\n",
        "  - [[-1, 6], 1, Concat, [1]] # cat backbone P4\n",
        "  - [-1, 3, C2f, [512]] # 12\n",
        "\n",
        "  - [-1, 1, nn.Upsample, [None, 2, \"nearest\"]]\n",
        "  - [[-1, 4], 1, Concat, [1]] # cat backbone P3\n",
        "  - [-1, 3, C2f, [256]] # 15 (P3/8-small)\n",
        "\n",
        "  - [-1, 1, Conv, [256, 3, 2]]\n",
        "  - [[-1, 12], 1, Concat, [1]] # cat head P4\n",
        "  - [-1, 3, C2f, [512]] # 18 (P4/16-medium)\n",
        "\n",
        "  - [-1, 1, Conv, [512, 3, 2]]\n",
        "  - [[-1, 9], 1, Concat, [1]] # cat head P5\n",
        "  - [-1, 3, C2f, [1024]] # 21 (P5/32-large)\n",
        "\n",
        "  - [[15, 18, 21], 1, Detect, [nc]] # Detect(P3, P4, P5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-29T13:34:23.819346Z",
          "iopub.status.busy": "2024-02-29T13:34:23.818659Z"
        },
        "id": "9opZmr81g4Aq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model.train(data=\"datatraffic.yaml\", cfg=\"my_yolov8.yaml\", epochs=200, iou=0.4, degrees=0.2, shear=0.2, flipud=0.5, mixup=0.3, augment=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOaBBOvxg39t",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import ultralytics\n",
        "model = ultralytics.YOLO('/content/runs/detect/train/weights/best.pt')\n",
        "model.export(format='edgetpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZGgmpt2g345"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30648,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
